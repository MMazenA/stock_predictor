{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_ta as ta\n",
    "import random\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(5973, 21)\n",
      "        Volume        Open        High         Low   Adj Close        RSI  \\\n",
      "5968  44838400  169.880005  171.339996  169.179993  169.300003  47.002910   \n",
      "5969  68169400  173.369995  176.029999  173.100006  173.500000  54.680984   \n",
      "5970  65934800  173.330002  174.990005  170.000000  170.330002  48.946490   \n",
      "5971  50383100  169.580002  172.710007  169.110001  169.300003  47.222447   \n",
      "5972  94214900  172.509995  173.419998  170.889999  173.029999  53.568120   \n",
      "\n",
      "            EMAF        EMAM        EMAS  EOM_14_100000000  ...  \\\n",
      "5968  169.684626  177.275329  178.109479          0.829386  ...   \n",
      "5969  170.047995  177.200570  178.048426          2.006966  ...   \n",
      "5970  170.074853  177.064520  177.946195          1.205201  ...   \n",
      "5971  170.001058  176.910767  177.831676         -1.727933  ...   \n",
      "5972  170.289528  176.833920  177.768078         -2.794841  ...   \n",
      "\n",
      "      STOCHd_14_3_3  MACD_12_26_9  MACDh_12_26_9  MACDs_12_26_9     ADX_14  \\\n",
      "5968      30.392146     -1.468319       0.261921      -1.730241  12.664285   \n",
      "5969      38.896666     -0.995047       0.588155      -1.583202  13.642521   \n",
      "5970      44.592283     -0.865787       0.573932      -1.439719  13.489043   \n",
      "5971      48.420480     -0.836814       0.482324      -1.319138  13.067018   \n",
      "5972      49.357099     -0.507028       0.649688      -1.156716  12.868359   \n",
      "\n",
      "         DMP_14     DMN_14    ADOSC_3_10  p_change  up_down  \n",
      "5968  28.265406  23.414340 -2.580845e+06  0.130005        1  \n",
      "5969  34.311975  19.996488 -2.043266e+07 -3.000000        0  \n",
      "5970  30.730132  24.394231 -4.408259e+07 -0.279999        0  \n",
      "5971  28.424742  24.418832 -6.408886e+07  0.520004        1  \n",
      "5972  27.477510  22.352134 -4.571157e+07 -3.289993        0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "ticker=\"AAPL\"\n",
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2025-10-31\"\n",
    "interval = \"1d\"\n",
    "\n",
    "\n",
    "# data = yf.download(\"AAPL\")\n",
    "data = yf.download(\n",
    "     ticker,\n",
    "     start=start_date,\n",
    "     end=end_date,\n",
    "    #  interval=interval,\n",
    " )\n",
    "df = pd.DataFrame(data[[\"Close\", \"Volume\", \"Open\", \"High\", \"Low\",\"Adj Close\"]])\n",
    "df['RSI'] = ta.rsi(data.Close,length=15)\n",
    "df['EMAF'] = ta.ema(data.Close,length=20)\n",
    "df['EMAM'] = ta.ema(data.Close,length=100)\n",
    "df['EMAS'] = ta.ema(data.Close,length=150)\n",
    "a = ta.volume.eom(data.High,data.Low,data.Close,data.Volume)\n",
    "df = df.join(a)\n",
    "\n",
    "a= ta.stoch(data.High,data.Low,data.Close,length=20)\n",
    "df = df.join(a)\n",
    "\n",
    "a= ta.macd(data.Close)\n",
    "df = df.join(a)\n",
    "\n",
    "a = ta.adx(df['High'], df['Low'], df['Close'], length = 14)\n",
    "df = df.join(a)\n",
    "\n",
    "a = ta.volume.adosc(df['High'], df['Low'], df['Close'],data.Volume)\n",
    "df = df.join(a)\n",
    "\n",
    "df[\"p_change\"] = df['Adj Close']-df.Open\n",
    "df[\"p_change\"] = df['p_change'].shift(-1)\n",
    "df[\"up_down\"] = [1 if df.p_change[i]>0 else 0 for i in range(len(df))]\n",
    "print(df.dropna(inplace=True,axis=0))\n",
    "df.reset_index(inplace=True)\n",
    "df.drop([\"Close\",\"Date\"],axis=1,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4778, 21)\n",
      "(1195, 21)\n",
      "torch.Size([4777, 1, 21])\n",
      "torch.Size([4777, 1, 21])\n",
      "torch.Size([1194, 1, 21])\n",
      "torch.Size([1194, 1, 21])\n"
     ]
    }
   ],
   "source": [
    "df_unscaled = df.copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df = np.array(df)\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = torch.tensor(data[i:i+seq_length], dtype=torch.float32)\n",
    "        target = torch.tensor(data[i+1:i+seq_length+1], dtype=torch.float32)\n",
    "        # seq = torch.transpose(seq,0,1)\n",
    "        # target = torch.transpose(target,0,1)\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return torch.stack(sequences).to(device), torch.stack(targets).to(device)\n",
    "\n",
    "seq_length = 1\n",
    "train_size = int(len(df) * 0.80) \n",
    "train_data= df[0:train_size]\n",
    "test_data = df[train_size:len(df)]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "X_train, Y_train = create_sequences(train_data, seq_length)\n",
    "X_test, Y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim,  embedding_dim\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "        \n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "         \n",
    "        \n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "              \n",
    "        x, (hidden, cell) = self.rnn1(x,(h_1, c_1))\n",
    "        \n",
    "        \n",
    "        return x, hidden , cell \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim ) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "       \n",
    "        hidden = hidden[2:3,:,:]\n",
    "        \n",
    "        #print(\"hidden size is\",hidden.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        #hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        hidden = hidden.repeat(1, src_len, 1)\n",
    "     \n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #print(\"encode_outputs size after permute is:\",encoder_outputs.size())\n",
    "        \n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "    \n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, seq_len,attention, input_dim=64, n_features=1,encoder_hidden_state = 512):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features =  input_dim, n_features\n",
    "        self.attention = attention \n",
    "        \n",
    "        self.rnn1 = nn.LSTM(\n",
    "          #input_size=1,\n",
    "          input_size= encoder_hidden_state + 1,  # Encoder Hidden State + One Previous input\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "        \n",
    "        \n",
    "      \n",
    "        self.output_layer = nn.Linear(self.hidden_dim * 2 , n_features)\n",
    "\n",
    "    def forward(self, x,input_hidden,input_cell,encoder_outputs):\n",
    "       \n",
    "        a = self.attention(input_hidden, encoder_outputs)\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "      \n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        \n",
    "     \n",
    "        x = x.reshape((1,1,1))\n",
    "       \n",
    "        \n",
    "        \n",
    "        rnn_input = torch.cat((x, weighted), dim = 2)\n",
    "       \n",
    "\n",
    "        #x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n",
    "        x, (hidden_n, cell_n) = self.rnn1(rnn_input,(input_hidden,input_cell))\n",
    "        \n",
    "        output = x.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        x = self.output_layer(torch.cat((output, weighted), dim = 1))\n",
    "        return x, hidden_n, cell_n\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64,output_length = 28):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        \n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim=embedding_dim).to(device)\n",
    "        self.attention = Attention(512,512)\n",
    "        self.output_length = output_length\n",
    "        self.decoder = AttentionDecoder(seq_len, self.attention, embedding_dim, n_features).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self,x, prev_y):\n",
    "        \n",
    "        encoder_output,hidden,cell = self.encoder(x)\n",
    "         \n",
    "        #Prepare place holder for decoder output\n",
    "        targets_ta = []\n",
    "        #prev_output become the next input to the LSTM cell\n",
    "        prev_output = prev_y\n",
    "        \n",
    "        #itearate over LSTM - according to the required output days\n",
    "        for out_days in range(self.output_length) :\n",
    "        \n",
    "            prev_x,prev_hidden,prev_cell = self.decoder(prev_output,hidden,cell,encoder_output)\n",
    "            hidden,cell = prev_hidden,prev_cell\n",
    "            prev_output = prev_x\n",
    "            \n",
    "            targets_ta.append(prev_x.reshape(1))\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "        targets = torch.stack(targets_ta)\n",
    "\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "model = Seq2Seq(seq_length,n_features,embedding_dim=512)\n",
    "model = model.to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-3,weight_decay=1e-5)\n",
    "criterion = torch.nn.MSELoss().to(device) \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, TrainX,Trainy,ValidX,Validy,seq_length, n_epochs):\n",
    "  \n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "    mb = master_bar(range(1, n_epochs + 1))\n",
    "\n",
    "    for epoch in mb:\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for i in progress_bar(range(TrainX.size()[0]),parent=mb):\n",
    "            seq_inp = TrainX[i,:,:].to(device)\n",
    "            seq_true = Trainy[i,:,:].to(device)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\n",
    "            \n",
    "            \n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in progress_bar(range(ValidX.size()[0]),parent=mb):\n",
    "                seq_inp = ValidX[i,:,:].to(device)\n",
    "                seq_true = Validy[i,:,:].to(device)\n",
    "        \n",
    "                seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\n",
    "               \n",
    "\n",
    "                loss = criterion(seq_pred, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"saved best model epoch:\",epoch,\"val loss is:\",val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "        scheduler.step()\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/30 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='4777' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/4777 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 1]' is invalid for input of size 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [102]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## Training only on 30 epochs to save GPU time \u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, TrainX, Trainy, ValidX, Validy, seq_length, n_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m seq_true \u001b[38;5;241m=\u001b[39m Trainy[i,:,:]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m seq_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_inp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(seq_pred, seq_true)\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, x, prev_y)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x, prev_y):\n\u001b[0;32m--> 148\u001b[0m     encoder_output,hidden,cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m#Prepare place holder for decoder output\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     targets_ta \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     h_1 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     24\u001b[0m     c_1 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1, 1]' is invalid for input of size 21"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  model,\n",
    "  X_train,Y_train,\n",
    "  X_test,Y_test,\n",
    "  10,\n",
    "  n_epochs=30, ## Training only on 30 epochs to save GPU time \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "def evaluate_model(self):\n",
    "    self.model.eval()\n",
    "    self.train_predictions = []\n",
    "    for seq in self.X_train:\n",
    "        with torch.no_grad():\n",
    "            self.model.hidden = (\n",
    "                torch.zeros(1, 1, self.model.hidden_size),\n",
    "                torch.zeros(1, 1, self.model.hidden_size),\n",
    "            )\n",
    "            y_pred = self.model(seq.unsqueeze(1))\n",
    "            predicted_close = y_pred[-1, 10:12] \n",
    "            self.train_predictions.append(predicted_close.numpy())\n",
    "    self.test_predictions = []\n",
    "    for seq in self.X_test:\n",
    "        with torch.no_grad():\n",
    "            self.model.hidden = (\n",
    "                torch.zeros(1, 1, self.model.hidden_size),\n",
    "                torch.zeros(1, 1, self.model.hidden_size),\n",
    "            )\n",
    "            y_pred = self.model(seq.unsqueeze(1))\n",
    "            predicted_close = y_pred[-1, 10:12]\n",
    "            self.test_predictions.append(predicted_close.numpy())\n",
    "    self.train_predictions = np.array(self.train_predictions)\n",
    "    self.y_train_inv = self.y_train.numpy().squeeze()\n",
    "    self.test_predictions = np.array(self.test_predictions)\n",
    "    self.y_test_inv = self.y_test.numpy().squeeze()\n",
    "    train_rmse = np.sqrt(mean_squared_error(self.y_train_inv, self.train_predictions))\n",
    "    test_rmse = np.sqrt(mean_squared_error(self.y_test_inv, self.test_predictions))\n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "\n",
    "def train_model(verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for seq, labels in zip(X_train, y_train):\n",
    "                optimizer.zero_grad()\n",
    "                model.hidden_cell = (\n",
    "                    torch.zeros(1, 1, model.hidden_size),\n",
    "                    torch.zeros(1, 1, model.hidden_size),\n",
    "                )\n",
    "                y_pred = model(seq.unsqueeze(1))\n",
    "\n",
    "                predicted_close = y_pred[-1][10:12]\n",
    "\n",
    "                true_close = labels\n",
    "                single_loss = criterion(predicted_close, true_close)\n",
    "                single_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch [{epoch+1}], Loss: {single_loss.item():.6f}\")\n",
    "                train_rmse, test_rsme = evaluate_model()\n",
    "                print(f\"  Train: {train_rmse:.4f}\")\n",
    "                print(f\"   Test: {test_rsme:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
